_name_or_path: ''
checkpoint_name_or_path: null
mindformers_version: 1.6.0
tokenizer_class: null
bos_token_id: 1
pad_token_id: 151643
eos_token_id: 151643
architectures: null
is_encoder_decoder: null
is_sample_acceleration: null
type: LlamaConfig
emb_dropout_prob: 0.0
compute_in_2d: true
rotary_pct: 1.0
rotary_emb_base: 1000000
kv_channels: 128
batch_size: 1
seq_length: 8192
vocab_size: 152064
hidden_size: 3584
num_layers: 2
num_heads: 28
max_position_embedding: 131072
intermediate_size: 18944
multiple_of: 256
n_kv_heads: 4
ffn_dim_multiplier: null
rms_norm_eps: 1.0e-06
qkv_concat: false
param_init_type: float32
embedding_init_type: float32
qkv_has_bias: true
attn_proj_has_bias: false
layernorm_compute_type: float32
softmax_compute_type: float16
rotary_dtype: float32
compute_dtype: bfloat16
residual_dtype: bfloat16
parallel_config:
  data_parallel: 1
  context_parallel: 1
  model_parallel: 4
  expert_parallel: 1
  pipeline_stage: 2
  micro_batch_num: 2
  seq_split_num: 1
  use_seq_parallel: false
  optimizer_shard: null
  gradient_aggregation_group: 4
  vocab_emb_dp: false
  recompute:
    recompute: true
    select_recompute: false
    parallel_optimizer_comm_recompute: false
    mp_comm_recompute: true
    recompute_slice_activation: false
    select_recompute_exclude: false
    select_comm_recompute_exclude: false
  context_parallel_algo: colossalai_cp
  ulysses_degree_in_cp: 1
  mem_coeff: 0.1
moe_config:
  expert_num: 1
  capacity_factor: 1.1
  aux_loss_factor: 0.05
  num_experts_chosen: 1
  expert_group_size: null
  group_wise_a2a: false
  comp_comm_parallel: false
  comp_comm_parallel_degree: 2
  save_token_distribution: false
  cur_layer: 0
  enable_cold_hot_expert: false
  update_step: 10000
  hot_expert_num: 0
  cold_token_percent: 1.0
  moe_module_name: ''
  routing_policy: TopkRouterV1
  norm_topk_prob: true
  enable_sdrop: false
  use_fused_ops_topkrouter: false
  router_dense_type: float32
  shared_expert_num: 0
  use_shared_expert_gating: false
  routed_scaling_factor: 1.0
  moe_intermediate_size: 1407
  n_group: null
  topk_group: null
  topk_method: greedy
  first_k_dense_replace: true
  aux_loss_types: []
  aux_loss_factors: []
  z_loss_factor: 0.0
  max_router_load: 131072
  balance_via_topk_bias: false
  topk_bias_update_rate: 0.0
  use_allgather_dispatcher: false
  moe_shared_expert_overlap: false
  expert_model_parallel: null
  use_gating_sigmoid: false
  enable_deredundency: false
  npu_nums_per_device: 1
  use_gmm: false
  enable_gmm_safe_tokens: false
  use_fused_ops_permute: false
  callback_moe_droprate: false
  dispatch_global_max_bs: 0
ignore_token_id: -100
use_past: false
extend_method: None
scaling_factor: 1.0
is_dynamic: false
use_rope_slice: false
use_flash_attention: true
use_ring_attention: false
use_attn_mask_compression: false
use_eod_attn_mask_compression: true
parallel_optimizer: false
fine_grain_interleave: 1
pp_interleave_num: 1
offset: 0
repetition_penalty: 1
max_decode_length: 512
top_k: 0
top_p: 0.8
do_sample: false
theta: 1000000.0
block_size: 32
num_blocks: 128
quant_config: null
tie_word_embeddings: false
llm_backend: ''
parallel_decoding_params: null
fused_rms_norm: true
init_method_std: 0.01
input_sliced_sig: false
rmsnorm_compute_2d: false
chunk_prefill: false
return_hidden_states: false
calculate_per_token_loss: false
start_stage: 0
stage_num: 0
model_name: llama
model_type: llama
