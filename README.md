<div align="center">

# MindRLHF

[![license](https://img.shields.io/github/license/mindspore-lab/mindrlhf.svg)](https://github.com/mindspore-lab/mindrlhf/blob/main/LICENSE.md)
[![open issues](https://img.shields.io/github/issues/mindspore-lab/mindrlhf)](https://github.com/mindspore-lab/mindrlhf/issues)
[![PRs](https://img.shields.io/badge/PRs-welcome-pink.svg)](https://github.com/mindspore-lab/mindrlhf/pulls)
[![Code style: autopep8](https://img.shields.io/badge/code_style-autopep8-blue)](https://github.com/hhatto/autopep8)

English | [中文](README_CN.md)

[Introduction](#introduction) |
[Installation](#installation) |
[Supported Models](#supported-models) |
[Get Started](#get-started) |
[Contributions](#Contributions) |
[License](#License)

</div>

# Introduction

OPENAI's [ChatGPT](https://openai.com/blog/chatgpt) has demonstrated astonishing natural language processing capabilities, opening the door to universal artificial intelligence. Its exceptional performance is closely tied to the [Reinforcement Learning from Human Feedback](https://openai.com/research/learning-from-human-preferences) (RLHF) algorithm. In its predecessor, [InstructGPT](https://openai.com/research/instruction-following), RLHF was used to collect human feedback and generate content that better aligns with human cognition and values, thus compensating for potential cognitive biases in large models.

MindSpore RLHF (MindRLHF) is based on the [MindSpore](https://gitee.com/mindspore/mindspore) and utilizes the framework's capabilities for large model parallel training, inference, and deployment to help customers quickly train and deploy RLHF algorithm processes with models that have billions or trillions of parameters.

The MindRLHF learning process consists of three stages:

* Stage 1: Supervised fine-tuning.
* Stage 2: Reward model training.
* Stage 3: Reinforcement learning training.

MindRLHF integrates the rich model library of the [MindFormers](https://github.com/mindspore-lab/mindformers), providing fine-tuning processes for basic models such as GPT-2.

Fully inheriting the parallel interface of MindSpore, MindRLHF can easily deploy models to the training cluster with just one click, enabling training and inference of large models.

To improve inference performance, MindRLHF integrates `incremental inference`, which is known as `K-V cache` or `state reuse` and can achieve more than a 30% improvement in inference performance compared to full inference.

MindRLHF architecture diagram is as follows:

![framework](https://github.com/mindspore-lab/mindrlhf/blob/master/images/framework.jpg)

## Installation

Current version `0.3.0` can be used directly.

There are some requirements for MindRLHF:

|  requirements   | version |
|  ----   |---------|
| MindSpore    | r2.7    |
| Mindformers | dev     |

The framework that the current version relies on:

| requirements     | version |
|------------------|------------------------------------------------------------|
| Firmware&Drivers | 24.1.RC3.3                                                 |
| CANN             | 8.1                                                        |
| Python           | 3.10                                                       |
| MindSpore        | master, commit id：8f35b18d992cacea735567ab011e91f83a074731 |
| MindFormers      | dev, commit id：6a52b43    |
| MindRLHF         | master, commit id：0cd0874559d5658e5987ba11718a920384691c59 |
| vLLM             | master, commit id：8f35b18d992cacea735567ab011e91f83a074731 |
| vLLM_MindSpore   | dev, commit id：6a52b43    |
| msadapter        | master, commit id：0cd0874559d5658e5987ba11718a920384691c59 |
| mindspore_gs     | master, commit id：0cd0874559d5658e5987ba11718a920384691c59 |

You can directly run the bash install.sh installation dependencies, and if the installation fails, you can refer to the [Installation Guide](docs/install). MindRLHF and MindFormers need to be used by specifying the PYTHONPATH.

## Supported Models

Current version of MindRLHF: `0.5.0`

The current version integrates GPT2.5(7B/32B), DeepSeekV3(671B) models, and users can explore these two models. In the future, we will provide more models such as Qwen3, etc. To help users quickly implement their own applications. The specific supported list is shown below:

Table 1： The models and scales supported in MindRLHF
|  Models   |  Qwen2_5 | DeepSeekV3 |
|  ----     |  ----   |  ----   |
| Scales    | 7B/32B    | 671B    |
| Parallel  | Y            |   Y       |
| Device    | NPU          |   NPU     |

The support of models for different training stages is shown in the following table:

Table 2： The models and stages supported in MindRLHF
| Train type                 | Qwen2_5  | DeepSeek V3 |
|----------------------------------|----------|-------------|
| [GRPO](examples/grpo)      | ✅        | ✅           |

In the future, we will integrate more models such as Qwen3, etc.

## Get Started

* Reward model training: a `Qwen2.5` based reward model training tutorial is listed in 'examples'.

* RLHF fine-tuning: here is an example for RLHF fine-tuning in `MindRLHF`:

```python
grpo_config = GRPOConfig()
sft_model_config_infer = LlamaConfig(**sft_config_infer.model.model_config)
sft_model_config_train = LlamaConfig(**sft_config_train.model.model_config)
ref_model_config = LlamaConfig(**ref_config.model.model_config)
tokenizer = Qwen2Tokenizer(args.vocab_path, args.merges_file_path, add_bos_token=False, add_eos_token=False)
trainer = GRPOTrainer(
    grpo_config=grpo_config,
    sft_model_config_infer=sft_model_config_infer,
    sft_model_config_train=sft_model_config_train,
    ref_model_config=ref_model_config,
    reward_funcs=[accuracy_reward, format_reward],
    tokenizer=tokenizer,
)
grpo_with_grad = init_grpo_network_and_optimizer(trainer)
for n in range(grpo_config.epochs):
    steps = trainer.prompt_dataset.get_dataset_size() // trainer.prompt_dataset.get_batch_size()
    for i in range(steps):
        trainer.make_experience(num_generations=grpo_config.num_generations, rank_id=rank_id)
        dataset = init_grpo_dataset(trainer)
        trainer.train(grpo_with_grad, dataset)
```

## Contribution

Welcome to the community. You can refer to the MindSpore contribution requirements on the Contributor Wiki.

## License

Apache 2.0 License.
