<div align="center">

# MindRLHF

[![license](https://img.shields.io/github/license/mindspore-lab/mindrlhf.svg)](https://github.com/mindspore-lab/mindrlhf/blob/main/LICENSE.md)
[![open issues](https://img.shields.io/github/issues/mindspore-lab/mindrlhf)](https://github.com/mindspore-lab/mindrlhf/issues)
[![PRs](https://img.shields.io/badge/PRs-welcome-pink.svg)](https://github.com/mindspore-lab/mindrlhf/pulls)
[![Code style: autopep8](https://img.shields.io/badge/code_style-autopep8-blue)](https://github.com/hhatto/autopep8)

[English](README.md) | ä¸­æ–‡

[ç®€ä»‹](#ç®€ä»‹) |
[æ–°é—»](#æ–°é—») |
[å®‰è£…](#å®‰è£…) |
[æ”¯æŒåˆ—è¡¨](#æ”¯æŒåˆ—è¡¨) |
[å¿«é€Ÿå…¥é—¨](#å¿«é€Ÿå…¥é—¨) |
[è´¡çŒ®](#è´¡çŒ®) |
[è®¸å¯è¯](#è®¸å¯è¯)

</div>

## ç®€ä»‹

OPENAIçš„[ChatGPT](https://openai.com/blog/chatgpt)åœ¨è‡ªç„¶è¯­è¨€æ–¹é¢è¡¨ç°å‡ºäº†ä»¤äººéœ‡æƒŠçš„æ•ˆæœï¼Œå¼€å¯äº†é€šç”¨äººå·¥æ™ºèƒ½çš„åºå¹•,å®ƒçš„ä¼˜ç§€è¡¨ç°ï¼Œä¸ RLHFï¼ˆ[Reinforcement Learning from Human Feedback](https://openai.com/research/learning-from-human-preferences)ï¼‰ç®—æ³•å¯†ä¸å¯åˆ†ã€‚

`MindSpore RLHF`ï¼ˆç®€ç§° `MindRLHF`ï¼‰ä»¥[MindSpore](https://gitee.com/mindspore/mindspore)ä½œä¸ºåŸºç¡€æ¡†æ¶ï¼Œåˆ©ç”¨æ¡†æ¶å…·å¤‡çš„å¤§æ¨¡å‹å¹¶è¡Œè®­ç»ƒã€æ¨ç†ã€éƒ¨ç½²ç­‰èƒ½åŠ›ï¼ŒåŠ©åŠ›å®¢æˆ·å¿«é€Ÿè®­ç»ƒåŠéƒ¨ç½²å¸¦æœ‰ç™¾äº¿ã€åƒäº¿çº§åˆ«åŸºç¡€æ¨¡å‹çš„RLHFç®—æ³•æµç¨‹ã€‚MindRLHFåŒ…å«3ä¸ªé˜¶æ®µçš„å­¦ä¹ æµç¨‹ï¼š

* é˜¶æ®µ1ï¼š é¢„è®­ç»ƒæ¨¡å‹è®­ç»ƒ
* é˜¶æ®µ2ï¼š å¥–åŠ±æ¨¡å‹è®­ç»ƒ
* é˜¶æ®µ3ï¼š å¼ºåŒ–å­¦ä¹ è®­ç»ƒ

MindRLHFé›†æˆäº†å¤§æ¨¡å‹å¥—ä»¶[MindFormers](https://github.com/mindspore-lab/mindformers)ä¸­ä¸°å¯Œçš„æ¨¡å‹åº“ï¼Œ æä¾›äº†`Qwen2_5`ã€`Glm4`ç­‰åŸºç¡€æ¨¡å‹çš„å¾®è°ƒæµç¨‹ã€‚MindRLHFå®Œå…¨ç»§æ‰¿MindSporeçš„å¹¶è¡Œæ¥å£ï¼Œå¯ä»¥ä¸€é”®å°†æ¨¡å‹éƒ¨ç½²åˆ°è®­ç»ƒé›†ç¾¤ä¸Šï¼Œå¼€å¯å¤§æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†ã€‚

### ç‰¹æ€§

* MindRLHFä¸­é›†æˆäº†`å¢é‡æ¨ç†`ä»¥æå‡æ¨ç†æ€§èƒ½ï¼Œé€šè¿‡çŠ¶æ€å¤ç”¨ï¼Œç›¸æ¯”äºå…¨é‡æ¨ç†ï¼Œæ¨ç†æ€§èƒ½å¯æå‡`30%`ä»¥ä¸Šã€‚
* MindRLHFç»„ä»¶åŒ–è§£è€¦è®­ç»ƒæµç¨‹ä¸æ¨¡å‹å®šä¹‰ï¼Œæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰ä¿®æ”¹æ¨¡å‹ç»“æ„ã€å¥–åŠ±å‡½æ•°ã€è®­ç»ƒè¶…å‚ç­‰ã€‚
* é€šè¿‡è®­ç»ƒå’Œæ¨ç†æƒé‡åœ¨çº¿å¿«é€Ÿè‡ªåŠ¨é‡æ’ï¼ŒMindRLHFå®ç°äº†è®­æ¨å…±éƒ¨ç½²ï¼Œé¿å…æƒé‡æ–‡ä»¶è½ç›˜æ“ä½œï¼ŒèŠ‚çœç¦»çº¿è½¬æ¢ä¿å­˜æƒé‡æ–‡ä»¶çš„æ—¶é—´å¼€é”€ã€‚
* é€šè¿‡å¼‚æ„å†…å­˜SwapæŠ€æœ¯ï¼ŒMindRLHFæŒ‰éœ€åŠ è½½æ¨¡å‹è‡³æ˜¾å­˜ï¼Œé¿å…è®­ç»ƒå’Œæ¨ç†çš„æƒé‡åŒæ—¶å­˜åœ¨ï¼Œæ”¯æŒæ›´å¤§è§„æ¨¡æ¨¡å‹çš„è®­ç»ƒä»»åŠ¡ã€‚

## æ–°é—»

* [2025.3] ğŸš€ğŸš€ğŸš€ MindRLHFå·²å®ç°[DeepSeek V3 + GRPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒå…¨æµç¨‹æ”¯æŒ](https://mp.weixin.qq.com/s?__biz=MzkxMTM2MjMzNg==&mid=2247630704&idx=1&sn=e340c4170b1ea13865fcf6325ef07d92&chksm=c0fbe4f3f595138cbf561f7f840a99a4d38b2237b67faadd3d79298365e0f42e589629bce868&scene=126&sessionid=1743684839#rd) ğŸš€ğŸš€ğŸš€
* [2025.2] ğŸš€ğŸš€ğŸš€ MindRLHFä¸é¹åŸå®éªŒå®¤åŸºäºQwen2.5-7Bã€32Bæ‰“é€š[GRPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒå…¨æµç¨‹](https://mp.weixin.qq.com/s/up7vYWn3NmNiW9KA_n4P4w) ğŸš€ğŸš€ğŸš€

## å®‰è£…

MindRLHFæ”¯æŒæºç å®‰è£…æˆ–é€šè¿‡Dockeré•œåƒå®‰è£…ã€‚

### æºç å®‰è£…

å½“å‰ç‰ˆæœ¬`0.3.0`æ”¯æŒæºç å®‰è£…ï¼š

```bash
git clone https://gitee.com/mindspore/mindrlhf.git
cd mindrlhf
pip install -e .
```

å½“å‰ç‰ˆæœ¬æ‰€ä¾èµ–æ¡†æ¶:

| ä¾èµ– | ç‰ˆæœ¬ |
|------|----|
| å›ºä»¶&é©±åŠ¨ | 24.1.RC3.3 |
| CANN | 8.0 |
| Python | 3.10 |
| MindSpore | master, commit idï¼š94ac228bae9cd6d0f00b4ce8d5857773799c4f26 |
| MindFormers | dev, commit idï¼ša9fde06e1fafedb4e09b7334f7b2d9f219bf8ef8 |
| MindRLHF | master, commit idï¼š90977955470ea04f0e2256d6b73bc71ff62cf092 |

### é•œåƒå®‰è£…

ç”¨æˆ·ä¹Ÿå¯ä»¥é€šè¿‡Dockeré•œåƒä¸€é”®å®‰è£…MindRLHFå’Œæ‰€æœ‰ç›¸å…³ä¾èµ–ï¼š

1. ä¸‹è½½[Dockeré•œåƒ](https://openi.pcl.ac.cn/PCL-Reasoner/GRPO-Training-Container.git)(æ¨èä½¿ç”¨`git LFS`ä¸‹è½½)ã€‚
2. åŸºäºé•œåƒåˆ›å»ºå®¹å™¨

    ```bash
    docker run -itd  --privileged  --network=host \
           --shm-size 500g \
           --device=/dev/davinci0 \
           --device=/dev/davinci1 \
           --device=/dev/davinci2 \
           --device=/dev/davinci3 \
           --device=/dev/davinci4 \
           --device=/dev/davinci5 \
           --device=/dev/davinci6 \
           --device=/dev/davinci7 \
           --device=/dev/davinci_manager \
           --device=/dev/hisi_hdc \
           --device /dev/devmm_svm \
           -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
           -v /usr/local/Ascend/firmware:/usr/local/Ascend/firmware \
           -v /usr/local/sbin/npu-smi:/usr/local/sbin/npu-smi \
           -v /usr/local/sbin:/usr/local/sbin \
           -v /etc/hccn.conf:/etc/hccn.conf \
           CONTAINER_NAME:TAG \
           bash
    ```

3. è¿›å…¥å®¹å™¨

    ```bash
    docker exec -it CONTAINER_ID bash
    ```

## æ”¯æŒåˆ—è¡¨

å½“å‰ç‰ˆæœ¬é›†æˆäº†`Qwen2_5`ã€`DeepSeek V3`ã€`Glm4`ç­‰æ¨¡å‹ï¼Œç”¨æˆ·å¯ä»¥åŸºäºè¿™äº›æ¨¡å‹è¿›è¡Œæ¢ç´¢ã€‚æœªæ¥ï¼Œæˆ‘ä»¬å°†æä¾›æ›´å¤šæ¨¡å‹å¦‚`LLAMA3`ã€`Pangu`ç­‰ï¼Œå¸®åŠ©ç”¨æˆ·å¿«é€Ÿå®ç°è‡ªå·±çš„åº”ç”¨ã€‚

MindSpore RLHFä¸­ä¸åŒæ¨¡å‹å¯¹ä¸åŒè®­ç»ƒé˜¶æ®µçš„æ”¯æŒæƒ…å†µå¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š

è¡¨ 1ï¼š MindSpore RLHFæ”¯æŒçš„æ¨¡å‹å’Œé˜¶æ®µ

| è®­ç»ƒé˜¶æ®µ                             | Qwen2_5  | DeepSeek V3 | Glm4 |
|----------------------------------|----------|-------------|------|
| [å¥–åŠ±æ¨¡å‹è®­ç»ƒ](examples/reward_model)  | âŒ        | âŒ           | âŒ    |
| [DPOåå¥½å¾®è°ƒè®­ç»ƒ](examples/dpo)        | âœ…        | âŒ           | âœ…    |
| [PPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒ](examples/ppo)        | âœ…        | âŒ           | âŒ    |
| [GRPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒ](examples/grpo)      | âœ…        | âœ…           | âŒ    |

## å¿«é€Ÿå…¥é—¨

* å¥–åŠ±æ¨¡å‹è®­ç»ƒ: åœ¨[`examples/reward_model_train_tutorial`](examples/reward_model)æ–‡ä»¶å¤¹ä¸­å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨`Llama2`ã€`Glm4`æ¨¡å‹è¿›è¡Œå¥–åŠ±æ¨¡å‹å¾®è°ƒçš„è¿‡ç¨‹ã€‚

* DPOåå¥½å¾®è°ƒè®­ç»ƒï¼šåœ¨[`examples/dpo`](examples/dpo)æ–‡ä»¶å¤¹ä¸­å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨`GLM4`ã€`Qwen2`å’Œ`Qwen2.5`æ¨¡å‹è¿›è¡ŒDPOåå¥½å¾®è°ƒè®­ç»ƒçš„è¿‡ç¨‹ã€‚

* PPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒ: åœ¨[`examples/ppo/qwen_ppo_tutorial/README.md`](examples/ppo/qwen_ppo_tutorial/README.md)ä¸­å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨MindRLHFä¸­çš„`PPOTrainer`ç»„ä»¶ä½¿ç”¨`Qwen2.5`æ¨¡å‹è¿›è¡ŒPPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ¨¡å‹ä¸æ•°æ®é›†è·å¾—ã€
  æ¨¡å‹åˆ‡åˆ†ã€æ•°æ®é›†å¤„ç†ã€é…ç½®è®¾ç½®ä»¥åŠæ‹‰èµ·è®­ç»ƒä»»åŠ¡ã€‚

ä¸‹é¢æ˜¯`MindRLHF`ä¸­ä½¿ç”¨`PPOTrainer`æ‹‰èµ·è®­ç»ƒä»»åŠ¡çš„ä¸»è¦ä»£ç æ­¥éª¤ã€‚

   ```python
   ppo_config, sft_model_config, ref_model_config, critic_model_config, rm_model_config = init_configs(
       args)
   trainer = PPOTrainer(ppo_config=ppo_config, sft_model_config=sft_model_config, ref_model_config=ref_model_config,
                           critic_model_config=critic_model_config, rm_model_config=rm_model_config)
   ppo_with_grad = init_network_and_optimizer(trainer)
   for epoch in range(ppo_config.epochs):
       # sampling
       trainer.make_experience(num_rollouts=ppo_config.num_rollouts)
       dataset = init_ppo_dataset(trainer)
       # use data sink to accelerate
       trainer.train(ppo_with_grad, dataset, epoch)
       trainer.save_checkpoint(rank_id, epoch)
   ```

* GRPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒ: åœ¨[`examples/grpo/qwen_grpo_tutorial/README.md`](examples/grpo/qwen_grpo_tutorial/README.md)ä¸­å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨MindRLHFä¸­çš„`GRPOOTrainer`ç»„ä»¶ä½¿ç”¨`Qwen2.5`æ¨¡å‹è¿›è¡ŒGRPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ¨¡å‹ä¸æ•°æ®é›†è·å¾—ã€
  æ¨¡å‹åˆ‡åˆ†ã€æ•°æ®é›†å¤„ç†ã€é…ç½®è®¾ç½®ä»¥åŠæ‹‰èµ·è®­ç»ƒä»»åŠ¡ã€‚

ä¸‹é¢æ˜¯`MindRLHF`ä¸­ä½¿ç”¨`GRPOTrainer`æ‹‰èµ·è®­ç»ƒä»»åŠ¡çš„ä¸»è¦ä»£ç æ­¥éª¤ã€‚

   ```python
   grpo_config = GRPOConfig()
   sft_model_config_infer = LlamaConfig(**sft_config_infer.model.model_config)
   sft_model_config_train = LlamaConfig(**sft_config_train.model.model_config)
   ref_model_config = LlamaConfig(**ref_config.model.model_config)
   tokenizer = Qwen2Tokenizer(args.vocab_path, args.merges_file_path, add_bos_token=False, add_eos_token=False)
   trainer = GRPOTrainer(
       grpo_config=grpo_config,
       sft_model_config_infer=sft_model_config_infer,
       sft_model_config_train=sft_model_config_train,
       ref_model_config=ref_model_config,
       reward_funcs=[accuracy_reward, format_reward],
       tokenizer=tokenizer,
   )
   grpo_with_grad = init_grpo_network_and_optimizer(trainer)
   for n in range(grpo_config.epochs):
       steps = trainer.prompt_dataset.get_dataset_size() // trainer.prompt_dataset.get_batch_size()
       for i in range(steps):
           trainer.make_experience(num_generations=grpo_config.num_generations, rank_id=rank_id)
           dataset = init_grpo_dataset(trainer)
           trainer.train(grpo_with_grad, dataset)
   ```

## è´¡çŒ®

æ¬¢è¿å‚ä¸ç¤¾åŒºè´¡çŒ®ï¼Œå¯å‚è€ƒMindSporeè´¡çŒ®è¦æ±‚Contributor Wikiã€‚

## è®¸å¯è¯

Apache 2.0è®¸å¯è¯
